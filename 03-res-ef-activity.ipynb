{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with HTRC Extracted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will get you up-and-running with the HTRC Extracted Features dataset. Learn more about the data: https://wiki.htrc.illinois.edu/x/GoA5Ag\n",
    "\n",
    "The code and instructions used in this notebook combine elements from a Programming Historian lesson called \"Text Mining in Python through the HTRC Feature Reader\" (https://programminghistorian.org/en/lessons/text-mining-with-extracted-features) and the Berkeley Data Science Module, \"Library-HTRC\" (https://github.com/ds-modules/Library-HTRC).\n",
    "\n",
    "But first, an introduction to the environment in which you are reading this, Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Jupyter notebooks\n",
    "\n",
    "Before diving into the main focus of this notebook, it is helpful to first review some basics of Jupyter--the system in which you're readng this and will edit and execute the code below.\n",
    "\n",
    "Jupyter is one of many execution environments for Python.  Its specific vision of the world is that work is done iteratively, with an active session. There are two main types of Python environments:\n",
    "\n",
    "* \"Script-like\" execution means that you write down all the code in a file, and that entire file is run in order. \n",
    "\n",
    "* \"Interpreter-like\" execution means that you type in commands one at a time. The session pauses after each, waiting for the next command. This is really similar to how the command line is run.\n",
    "\n",
    "Jupyter is a hybrid of both those things.  Notebooks are composed of cells.  Then the cells are executed like mini scripts.  This gives you the advantage of keeping the session alive so you don't have to repeat loading data, etc., and the advantage of being able to execute multiple lines of code at the same time.\n",
    "\n",
    "Jupyter is powerful, but there are a few traps. Let's get comfortable with cells first. Try running this code cell by either pressing ```Shift``` + ```Enter``` or clicking on the \"Run\" icon in the toolbar below the Jupyter menus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensure that this cell is active, you can do that by clicking inside here.\")\n",
    "print(\"Press shift+enter to execute this cell.\")\n",
    "print(\"Try using the right shift and return at the same time, with one hand.\")\n",
    "print(\"You can also press the 'play' button at the top.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above contains 4 print statements. Jupyter supports aspects of a script-like environment, which means it will \"read\" (and execute) code in a cell as a human would: from top to bottom, each line from left to right. Knowing this, we should see the statements print in this same order.\n",
    "\n",
    "Let's try more complex code to better illustrate this aspect of the Jupyter environment. Try running this code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The book, \" + title + \", by \" + author + \", was published in \" + str(year_published) + \".\")\n",
    "\n",
    "title = \"Jupyter and You\"\n",
    "author = \"Human, A.\"\n",
    "year_published = 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened when you ran this code?** \n",
    "\n",
    "**Can you fix the problems in the code and get the cell to run successfully?**\n",
    "\n",
    "We also get a glimpse of how Jupyter (and Python) handle errors. Pay close attention to error messages! They often contain information about what went wrong and hints to solutions. The error message will also include an arrow that will point to the exact line in the code that is causing the error (here our first print statement) and a sparse explanation for the error will appear at the bottom of the message. Though these messages are not always understandable and can be especially opaque if you're new to Python, a quick Google search will often provide information about how to fix or overcome the error, even if you do not understand the message itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no error message pops up, your cell ran successfully. Sometimes this will mean something is ```printed``` or ```returned``` (both terms that mean something pops up below the cell). But sometimes nothing new will appear. You can confirm a cell is running or has finished running by looking at the brackets to the top left of the cell. If there is an asterisk ```[*]``` in the brackets, then the cell is currently running. If there is a number ```[14]```, it has completed. If it is blank ```[]```, it has not yet been run. Note: a cell will still have a number in the brackets even if it threw an error!\n",
    "\n",
    "For cells that take longer to run, you may find yourself tabbing out of the Jupyter window. You can track progress of the cells by making note of the icon on the left of the tab. If it's a book, the notebook has no running cells. Conversely, if it's an hour glass, there are cells still running.\n",
    "\n",
    "Run this cell (feel free to adjust the number of seconds to \"sleep\", or wait, each loop by changing the ```1``` in ```time.sleep(1)``` to another integer) to notice these indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = 0\n",
    "\n",
    "for n in range(5):\n",
    "    time.sleep(1)\n",
    "    print(x)\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments in code\n",
    "\n",
    "Above we also see a feature of Python, and coding languages in general--a comment. Comments in Python are denoted with ```#``` and with a contrasting color to code. Python will not interpret the characters after the ```#``` as executable code, but instead as plain text. Comments are used to help contextualize and document choices that are made in code, as well as provide metadata and information about the code and its provenance. For instance, it is common to see informal citations in code comments, as well as text explaining possible limitations or variances of code. It is generally considered a best practice to add comments to (sometimes referred to as \"comment out\") code, especially with an eye to optimizing its utility for others. To add a comment in Python, you can simply type ```#``` and then your comment afterwards, or you can convert code into a comment by highlighting a selection of words with your mouse, and then pressing ```Command``` + ```/```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Jupyter pain points\n",
    "\n",
    "Flexible systems help you approach tasks in many different ways, but also allow us to cause ourselves problems in many different ways. Troubleshooting and Google/Stack Overflow searching will go a very long way to overcoming problems, but Jupyter has some common pain points most users run into. Here are some tips for overcoming them:\n",
    "\n",
    "* Jupyter allows you to evaluate cells out of order, but you should only do them in order--top to bottom.\n",
    "* In Python, we must import libraries (code that we or others have written for people to use) that we want to use in our notebook. We do this using import statements with the syntax ```import <library name>```. Running cells out of order can cause you to skip import statements, which will cause errors when running code that uses the library before Python has been told to import it.\n",
    "* If you are getting errors that make no sense, sometimes going back to the top and starting over fixes it.\n",
    "\n",
    "There are lots of good resources and tutorials available for Jupyter, Python and various libraries that you may wish to use. You may be interested in exploring some of them:\n",
    "\n",
    "* Quick Jupyter intro: https://github.com/sgsinclair/alta/blob/master/ipynb/GettingStarted.ipynb\n",
    "* Comprehensive Jupyter tutorial: https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "* Pandas tutorial: https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python\n",
    "* Python intro: https://www.datacamp.com/courses/intro-to-python-for-data-science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and reading in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need to import the Python modules we'll use throughout this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "import pandas \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted Featuers files are originally formatted in JSON notation and compressed; you'll notice the file format is '.json.bz2'. The FeatureReader library is able to work with the files in that format with needing to decompress the files.\n",
    "\n",
    "Within the library, there is a **FeatureReader object** that is used for loading the dataset files and making sense of them. It returns a **Volume object** for each file. A Volume is a representation of a single item in HathiTrust, for example a book or other textual work. From the Volume, you can access features about the work. To drill down to the features derived from individual pages, use the **Page object**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to get the FeatureReader ready to use by pointing it to the file paths for the sample Extracted Features files we are using in this notebook. The files are in directory called 'ef-data'. We'll be using the Extracted Features files four our workset of all 30 volumes of Josiah Conder's _The Modern Traveller_.\n",
    " \n",
    "With fr = FeatureReader(paths) below, the FeatureReader is initialized, meaning it is ready to use. An initialized FeatureReader is holding references to the file paths that we gave it, and will load them into Volume objects when asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of file paths from ef-data directory and load the data in the Feature Reader\n",
    "\n",
    "file_paths = glob.glob('ef-data/*.bz2') # glob is a library that will search a file path and return \n",
    "                                        # files with a given extension (format)\n",
    "file_paths.sort(reverse=True)\n",
    "\n",
    "print(file_paths)\n",
    "    \n",
    "fr_modtrav = FeatureReader(file_paths)\n",
    "\n",
    "# If you have HTIDs handy, or don't want to fuss with crawling a directory for paths, you can also use a\n",
    "# list of HTIDs to create a FeatureReader object:\n",
    "\n",
    "# got_fr = FeatureReader(ids=[\"mdp.39015050507618\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what titles have been loaded as Volumes. Because this is a multi-volume series, each volume has the same basic title, so we'll also print the HathiTrust ID, the unique identifier for a given volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for vol in fr_modtrav.volumes():\n",
    "    print(vol.id + ': ' + vol.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and page structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond title and ID, let's see what information we can find about the volumes in our FeatureReader collection by using another method, ```.volumes```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_modtrav.volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that our FeatureReader object (or collection) has 30 paths, and the first and last paths are listed. This is a good sanity check to make sure all of the volumes we waanted to analyze are included in our collection.\n",
    "\n",
    "We can also call just one Volume at a time in order to examine its contents. In this example, we are taking the first file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vol = fr_modtrav.first()\n",
    "\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also use ```create_volume()``` and one of the paths we returned above to pick a volume from our FeatureReader collection directly, rather than just use ```.first( )```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vol = fr_modtrav.create_volume('ef-data/mdp.39015074624324.json.bz2')\n",
    "\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call the URL for the volume and CTRL-click it to find the corresponding item in the HathiTrust Digital Library (HTDL). \n",
    "\n",
    "**Note:** these volumes are in the public domain, so we will find that they are available for \"Full View\" in the HTDL. If they were still under copyright, we would be taken to a \"Limited View\" page. The Extracted Features dataset includes a snapshot of 15.7 million volumes from the HTDL and is agnostic to rights status, as the files represent data about the volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol.handle_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see what other metadata elements are available to you for each item in its corresponding Extracted Features file. Put your cursor between the period and the end parenthesis, and press tab. You can choose from the dropdown list. Then run the cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to access the first features of ```vol```, which is a table of total words for every single page. These can be accessed simply by calling ```vol.tokens_per_page()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol.tokens_per_page()\n",
    "\n",
    "# Show just the first few rows, so we can look at what it looks like\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a straightforward table of information, similar to what you would see in Excel or Google Spreadsheets. Listed in the table are page numbers and the count of words on each page. \n",
    "\n",
    "With only two dimensions, it is trivial to plot the number of words per page. The table structure holding the data has a plot method for data graphics. Without extra arguments, ```tokens.plot()``` will assume that you want a line chart with the page on the x-axis and word count on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tokens.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we get here? When we ran ```vol.tokens_per_page()```, it returned a Pandas DataFrame. This means that after setting tokens, we're no longer working with HTRC-specific code, just book data held in a common and very robust table-like construct from Pandas. ```tokens.head()``` used a DataFrame method to look at the first few rows of the dataset, and ```tokens.plot()``` uses a method from Pandas to visualize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Token List\n",
    "Another DataFrame accessible to us is `vol.tokenlist()`, which can be called to return section-, part-of-speech-, and word-specific details.\n",
    "\n",
    "Let's use this method to look at some words deeper into the book: from 1000th to 1100th row, skipping by 15, denoted by `[1000:1100:15]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = vol.tokenlist()\n",
    "tl[1000:1100:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the data is returned as a Pandas DataFrame. This time, there is much more information. The columns in bold are an index. Unlike the typical one-dimensional index seen before, here there are four dimensions to the index: page, section, token, and pos. This row says that for the 24th page, in the body section (i.e. ignoring any words in the header or footer), the word 'years' occurs 1 time as an plural noun. The part-of-speech tag for a plural noun, NNS, follows the Penn Treebank definition.\n",
    "\n",
    "You can sort of see this as nested information, moving beyond how we might normally work with tabular or spreadsheet data. The blank cells are areas where the data would have normally been duplicated.\n",
    "\n",
    "The HTRC Feature Reader refers to \"pages\" as the $n^{th}$ scanned image of the volume, not the actual number printed on the page. This is why, often, the first page may be the cover, or inside cover, or a blank page.\n",
    "\n",
    "Tokenlists can be retrieved with arguments -- or the stuff that goes inside the `()` -- that combine information by certain dimensions, such as case, POS, or page. For example, `case=False` specified that \"Jaguar\" and \"jaguar\" should be counted together. You may also notice that, by default, only `body` is returned, a default that can be overridden.\n",
    "\n",
    "Look at the following list of commands--can you guess what the output will look like?\n",
    "\n",
    "```python\n",
    "vol.tokenlist(case=False)\n",
    "vol.tokenlist(pos=False)\n",
    "vol.tokenlist(pages=False, case=False, pos=False)\n",
    "vol.tokenlist(section='header')\n",
    "vol.tokenlist(section='group')\n",
    "```\n",
    "**Try for yourself by adding these optional parameters between the parentheses below and observe how the output changes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.tokenlist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for these arguments are available in the code documentation for the Feature Reader or by running:\n",
    "\n",
    "```python\n",
    "vol.tokenlist?\n",
    "```\n",
    "This is a Python-wide functionality, and can be very helpful in understanding someone else's code! Any time you are wondering what a certain method does, you can add `?` after the method name and execute the code to get the documentation to pop up in your browser window.\n",
    "\n",
    "**Give this a shot below (and feel free to switch which method you're investigating using the `tab`-after-period trick!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.tokenlist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with DataFrames\n",
    "\n",
    "The Pandas DataFrame type returned by the HTRC Feature Reader is very malleable. To work with the tokenlist that you retrieved earlier (`tl`), three skills are particularily valuable:\n",
    "\n",
    "1. Selecting subsets by a condition\n",
    "2. Slicing by named row index\n",
    "3. Grouping and aggregating\n",
    "\n",
    "### Selecting Subsets of a DataFrame by a Condition\n",
    "\n",
    "Consider this example: *I only want to look at tokens that occur more than a hundred times in the book.* \n",
    "\n",
    "Remembering that the table-like output from the HTRC Feature Reader is a Pandas DataFrame, the way to pursue this goal is to learn to filter and subset DataFrames. Knowing how to do so is important for working with just the data that you need.\n",
    "\n",
    "To subset individual rows of a DataFrame, you can provide a series of True/False values to the DataFrame, formatted in square brackets. When True, the DataFrame returns that row; when False, the row is excluded from what is returned.\n",
    "\n",
    "To see this in context, first load a basic tokenlist without parts-of-speech or individual pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple = vol.tokenlist(pos=False, pages=False)\n",
    "# .sample(5) returns five (or any number included in the parentheses) random words from the full result\n",
    "tl_simple.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select just the relevant tokens, we need to look at each row and evaluate whether it matches the criteria that \"this token has a count greater than 100\". Let's try to convert that requirement to code.\n",
    "\n",
    "\"This token has a count\" means that we are concerned specifically with the 'count' column, which can be singled out from the `tl` table with `tl['count']`. \"greater than 100\" is formalized as `> 100`. Putting it together, try the following and see what you get:\n",
    "\n",
    "```python\n",
    "tl_simple['count'] > 100\n",
    "```\n",
    "\n",
    "It is a DataFrame of True/False values. Each value indicates whether the 'count' column in the corresponding row matches the criteria or not. We haven't selected a subset yet, we simply asked a question and were told for each row when the question was true or false.\n",
    "\n",
    "> You may wonder why section and token are still seen, even though 'count' was selected. These are part of the DataFrame **index**, so they're considered part of the information *about* that row rather than data *in* the row. You can convert the index to data columns with `reset_index()`. In this lesson we will keep the index intact, though there are advanced cases where there are benefits to resetting it.\n",
    "\n",
    "Armed with the True/False values of whether each token's ```count``` value is or isn't greater than 100, we can give those values to `tl_simple` in square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = tl_simple['count'] > 100\n",
    "tl_simple[matches].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can move the comparison straight into the square brackets, the more conventional equivalent of the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple[tl_simple['count'] > 100].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As might be expected, many of the tokens that occur very often are common words like \"she\" and \"and\", as well as various punctuation. \n",
    "\n",
    "Multiple conditions can be chained with `&` (and) or `|` (or), using regular brackets so that Python knows the order of operations. For example, words with a count greater than 150 *and* a count less than 200 are selected in this way. **Try adding your own number for minimum and maximum token occurrences:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple[(tl_simple['count'] > ADD NUMBER HERE) & (tl_simple['count'] < ADD LARGER NUMBER HERE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also slice a DataFrame by named row index. To practice this, first we'll creat a new token list, ```tl_all``` that includes all page sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_all = vol.tokenlist(section='all')\n",
    "tl_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add a word between the quotation marks to retrieve only pages where that word occurs. We are using the power of the DataFrame index to retrieve only the rows that match our criteria. **Try it out by adding a word:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pages = tl_all.loc[(slice(None), slice(None), 'ADD A WORD HERE!'),]\n",
    "query_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing DataFrames\n",
    "It is also possible to select a DataFrame subset by specifying the values of its index, a process called **slicing**. For example, you can ask, *\"give me all the verbs for pages 9-12\"*.\n",
    "\n",
    "In the DataFrame returned by `vol.tokenlist()`, page, section, token, and POS were part of the index (try the command `tl.index.names` to confirm). One can think of an index as the margin content of an Excel spreadsheet: the letters along the top and numbers along the left side are the indices. A cell can be referred to as A1, A2, B1... In Pandas, however, you can name these, so instead of A, B, C, or 1,2,3, columns and rows can be referred to by more descriptive names. You can also have multiple levels, so you're not bound by the two-dimensions of a table format. With a multi-indexed DataFrame, you can ask for `Page=24,section=Body, ...`.  One can think of an index as the margin notations in Excel (i.e. 1,2,3... and A,B,C,..), except it can be named and can have multiple levels.\n",
    "\n",
    "Slicing a DataFrame against a labelled index is done using `DataFrame.loc[]`. Try the following examples and see what is returned:\n",
    "\n",
    "- Select information from page 17: \n",
    "  - `tl.loc[(17),]`\n",
    "- Select 'body' section of page 17:\n",
    "  - `tl.loc[(17, 'body'),]`\n",
    "- Select counts of the word 'Euphrates' in the 'body' section of page 17:\n",
    "  - `tl.loc[(17, 'body', 'Euphrates'),]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this code cell to try out the above examples, or come up with your own\n",
    "\n",
    "tl.loc[(17,'body','Euphrates'),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The levels of the index are specified in order, so in this case the first value refers to 'page', then 'section', and so on. To skip specifying anything for an index level -- that is, to select everything for that level -- `slice(None)` can be used as a placeholder:\n",
    "\n",
    "- Select counts of the word 'Euphrates' for all pages and all page sections\n",
    "  - `tl.loc[(slice(None), slice(None), \"Euphrates\"),]`\n",
    "  \n",
    "Finally, it is possible to select multiple labels for a level of the index, with a list of labels (i.e. `['label1', 'label2']`) or a sequence covering everything from one value to another (i.e. `slice(start, end)`):\n",
    "\n",
    "- Select pages 37, 38, and 52\n",
    "  - `tl.loc[([37, 38, 52]),]`\n",
    "- Select all pages from 37 to 40\n",
    "  - `tl.loc[(slice(37, 40)),]`\n",
    "- Select counts for 'Euphrates' or 'Tigris' from all pages\n",
    "  - `tl.loc[(slice(None), slice(None), [\"Euphrates\", \"Tigris\"]),]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.loc[(slice(None), slice(None), [\"Euphrates\",\"Tigris\"]),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try substituting the token search terms \"Euphrates\" and \"Tigris\" above, or try out using slices to find information about other pages or sections. Let us know if you find something interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Sorting DataFrames \n",
    "A DataFrame can be sorted with `DataFrame.sort_values()`, specifying the column to sort by as the first argument. By default, sorting is done in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple.sort_values('count').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descending order is possible with the argument `ascending=False`, which puts the most common tokens at the top. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple.sort_values('count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Frequencies and Stopword lists\n",
    "\n",
    "Now we'll look at a look using relative frequencies. Relative frequencies are one way at looking at top words, through their proportional counts. Books have different lengths, so the nominal count of any given word will vary between books, so relative frequencies give us a way to compare two or more books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looking at one volume, let's start to explore the relative frequencies of tokens within the volume. \n",
    "\n",
    "The following cell will display the most common tokens (words or punctuation marks) in a given volume, alongside the number of times they appear. It will also calculate their relative frequencies (found by dividing the number of appearances over the total number of words in the book) and display the results in a DataFrame. The cell may take a few seconds to run because we're looping through every word in the volume!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol.tokenlist(pos=False, case=False, pages=False).sort_values('count', ascending=False)\n",
    "\n",
    "freqs = []\n",
    "for count in tokens['count']:\n",
    "    freqs.append(count/sum(tokens['count'])) # generating a frequency by volume for a given token \n",
    "                                             # by counting it's frequency in the volume, and dividing \n",
    "                                             # by total tokens in the volume\n",
    "    \n",
    "tokens['rel_frequency'] = freqs\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the most common tokens from the volume and their frequencies. The following cell outputs a bar plot using the ```matplotlib``` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we use a command that helps matplotlib plots better display in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Build a list of frequencies and a list of tokens.\n",
    "freqs_1, tokens_1 = [], []\n",
    "for i in range(15):  # top 15 words\n",
    "    freqs_1.append(freqs[i])\n",
    "    tokens_1.append(tokens.index.get_level_values('lowercase')[i])\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = numpy.arange(len(tokens_1))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_1)\n",
    "plt.xticks(x_ticks, tokens_1, rotation=90)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:20] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most common tokens are mostly punctuation and basic words that don't provide context. Let's see if we can narrow our search to gain some more relevant insight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a list of stopwords from the nltk library. Punctuation is in the string library. Let's import nltk and make the stopwords and punctuation accessible to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the nltk library and downloaded the stopwords module, we can look at what is included in each list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english') # making an easier-to-work-with list from the NLTK stopwords\n",
    "\n",
    "from string import punctuation\n",
    "punctuation = list(punctuation) # making a list again, as above\n",
    "\n",
    "print(f\"Stopwords: \\n {stopwords}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Punctuation: \\n {punctuation}\")\n",
    "\n",
    "# Since we imported the stopwords into a basic Python list, we can remove individual words/punctuation using \n",
    "# basic list method of .drop():\n",
    "\n",
    "# stopwords.drop('myself') \n",
    "\n",
    "# punctuation.drop('#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on stop words\n",
    "\n",
    "Notice above that there is optional code in comments that can be included to drop any of the punctuation or words from the stopword list. Feel free to modify each list however you see fit! Defining and removing stopwords will *always* be a decision that is unique to a particular project. There are no best practices for stopwords generally!\n",
    "\n",
    "With these lists of words/characters to ignore in our DataFrame, we can make a few tweaks to our plotting cell to remove the punctuation and display only those words not in our stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_filtered, tokens_filtered, i = [], [], 0\n",
    "\n",
    "while len(tokens_filtered) < 10:\n",
    "    if tokens.index.get_level_values('lowercase')[i] not in stopwords + punctuation:\n",
    "        freqs_filtered.append(freqs[i])\n",
    "        tokens_filtered.append(tokens.index.get_level_values('lowercase')[i])\n",
    "    i += 1\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = numpy.arange(len(freqs_filtered))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_filtered)\n",
    "plt.xticks(x_ticks, tokens_filtered, rotation=90)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:20] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens from all volumes in our set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how word frequencies compare across all the books in our samples. \n",
    "\n",
    "First we'll set-up a few functions. A [function](https://www.w3schools.com/python/python_functions.asp) is \"a block of code which only runs when it is called. You can pass data, known as parameters, into a function.\" The functions we're going to define, and thus enable their use simply by calling them by name, rather than rewriting code, are:\n",
    "\n",
    "* ```most_common_noun``` finds the most common noun in a volume, with adjustable parameters for minimum length. \n",
    "* ```frequency``` calculates the relative frequency of a token across all volumes in a FeatureReader collection, saving us the time of doing the calculation like in the above cell.\n",
    "* ```frequency_bar_plot``` is a visualization function to create a bar plot of relative frequencies for all volumes in our sample, so that we can easily track how word frequencies differ across titles.\n",
    "\n",
    "We'll be defining and using each of these individually, and then together, in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common nouns\n",
    "\n",
    "Let's define and use a function to see what the most common nouns in this work are by word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing the function used in the next box\n",
    "def most_common_noun(vol, word_length=2):   \n",
    "    # build a table of common nouns\n",
    "    tokens_1 = vol.tokenlist(pages=False, case=False) # create token list DF\n",
    "    nouns_only = tokens_1.loc[(slice(None), slice(None), ['NN']),] # slice token list DF to get DF with only nouns\n",
    "    top_nouns = nouns_only.sort_values('count', ascending=False) # sort the noun DF by highest count\n",
    "\n",
    "    token_index = top_nouns.index.get_level_values('lowercase') # creating a new DF with token (here noun) frequency \n",
    "                                                                # as index\n",
    "    \n",
    "    # choose the first token at least as long as word_length with non-alphabetical characters\n",
    "    for i in range(max(token_index.shape)):\n",
    "        if (len(token_index[i]) >= word_length):\n",
    "            if(\"'\", \"!\", \",\", \"?\" not in token_index[i]):\n",
    "                return token_index[i]\n",
    "    print('There is no noun of this length')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If no number is included as a parameter with a volume, the word length defaults to a minimum of 2. We know this because when defining the function, 2 is included as a default value for the parameter ```word length``` (as shown in the first line of the function code). We can also test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Most common noun >= 2 characters long: '{most_common_noun(vol)}'\")\n",
    "\n",
    "print(f\"Most common noun >= 10 characters long: '{most_common_noun(vol, 10)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try experimenting with other word lengths by adding a number below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_noun(vol, ADD A NUMBER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative token frequency\n",
    "The function `frequency()` returns a plot of the usage frequencies of the given word across all volumes in the given FeatureReader collection (which could include one or more volumes).\n",
    "\n",
    "**Note**: `frequency()` returns a dictionary entry of the form `{'word': frequency}`. For example, `frequency(fr_modtrav.first(), 'temple')` returns `{'temple': 0.00019}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing the function used in the next box\n",
    "\n",
    "def frequency(vol, word):\n",
    "    t1 = vol.tokenlist(pages=False, pos=False, case=False) # create a token list\n",
    "    token_index = t1[t1.index.get_level_values(\"lowercase\") == word] # change all tokens to lower case\n",
    "    \n",
    "    if len(token_index['count'])==0: # check if token is already counted, and add to dictionary, if not\n",
    "        return {word: 0}\n",
    "    \n",
    "    count = token_index['count'][0] # return the integer of the token's count\n",
    "    freq = count/sum(t1['count']) # calculate the frequency by dividing the token's count by total number of tokens\n",
    "    \n",
    "    return {word: float('%.5f' % freq)} # '%.5f' means it'll return a float rounded to 5 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out! Add a word between the quotes in the cell below to return its relative frequency in the volume assigned to ```vol```:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency(vol, 'ADD A WORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together\n",
    "\n",
    "The code below uses our new function ```frequency``` to return a plot of the realtive usage frequencies of a given word across all volumes in a given FeatureReader collection.\n",
    "\n",
    "**Note**: frequencies are given as percentages rather than true ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing the function used in the next box\n",
    "def frequency_bar_plot(word, FR_col):\n",
    "    freqs, titles = [], []\n",
    "    for vol in FR_col:\n",
    "        title = vol.title\n",
    "        short_title = title[:6] + (title[6:] and '..')\n",
    "        freqs.append(100*frequency(vol, word)[word]) # calling our frequency function\n",
    "        titles.append(short_title)\n",
    "        \n",
    "    # Format and plot the data -- this replicates the code we used outside of the function in our bar chart above\n",
    "    x_ticks = numpy.arange(len(titles))\n",
    "    plt.bar(x_ticks, freqs)\n",
    "    plt.xticks(x_ticks, titles, fontsize=10, rotation=90)\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('Volume title', fontsize=12)\n",
    "    plt.title('Frequency of \"' + word + '\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try adding different words to see their relative frequency in our sample!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bar_plot('ADD A WORD', fr_modtrav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's interesting, but since all our titles are the same, it's hard to make sense of the results. Let's try plotting relative frequency over time.\n",
    "\n",
    "The code below returns a DataFrame of relative frequencies, volume years, and page counts, along with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to visualize frequency by volume\n",
    "def frequency_by_vol(query_word, FR_col):\n",
    "    volumes = pandas.DataFrame()\n",
    "    ids, page_counts, query_freqs = [], [], []\n",
    "\n",
    "    for vol in FR_col:\n",
    "        ids.append(vol.id)\n",
    "        page_counts.append(int(vol.page_count))\n",
    "        query_freqs.append(100*frequency(vol, query_word)[query_word])\n",
    "    \n",
    "    volumes['id'], volumes['pages'], volumes['freq'] = ids, page_counts, query_freqs\n",
    "    volumes = volumes.sort_values('id')\n",
    "    \n",
    "    fig_plot = volumes.plot('id','freq', scalex=False)\n",
    "    tick_labels = tuple(volumes['id'])\n",
    "    x_max = int(max(plt.xticks()[0]))  # int() to convert numpy.int32 => int\n",
    "    x_min = int(min(plt.xticks()[0]))\n",
    "    plt.xticks(range(x_min, x_max), tick_labels, rotation=90) \n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('HTID', fontsize=12)\n",
    "    plt.title('Frequency of \"' + query_word + '\"', fontsize=14)\n",
    "    \n",
    "    return volumes.head(10)\n",
    "\n",
    "# A bonus function that returns and plots frequences for a FR collection by volume year\n",
    "def frequency_by_year(query_word, FR_col):\n",
    "    volumes = pandas.DataFrame()\n",
    "    years, page_counts, query_freqs = [], [], []\n",
    "\n",
    "    for vol in FR_col:\n",
    "        years.append(int(vol.year))\n",
    "        page_counts.append(int(vol.page_count))\n",
    "        query_freqs.append(100*frequency(vol, query_word)[query_word])\n",
    "    \n",
    "    volumes['year'], volumes['pages'], volumes['freq'] = years, page_counts, query_freqs\n",
    "    volumes = volumes.sort_values('year')\n",
    "    \n",
    "    # Set plot dimensions and labels\n",
    "    scatter_plot = volumes.plot.scatter('year', 'freq', color='black', s=50, fontsize=12)\n",
    "    plt.ylim(0-numpy.mean(query_freqs), max(query_freqs)+numpy.mean(query_freqs))\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.title('Frequency of \"' + query_word + '\"', fontsize=14)\n",
    "    \n",
    "    return volumes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again, add a word between the quotes to try different terms:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frequency_by_vol('ADD A WORD', fr_modtrav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making use of the structured file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly useful thing about the Extracted Features dataset is that the tokens in the extracted features files are part-of-speech tagged to differentiate homynyms like 'rose', which can be a name, a noun, and a verb.\n",
    "\n",
    "For each page, the data is also divided into a header, body, and footer section so that you can systematically remove headers or footers from your data if you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw the possibiity of drilling down to the part-of-speech tag earlier when we found the most frequently-occuring noun in a volume. Below, we will look for one part of speech, adjectives, in just the body of our volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pandas.IndexSlice\n",
    "vol = next(fr_modtrav.volumes())\n",
    "tl = vol.tokenlist(pages=False)\n",
    "tl.index = tl.index.droplevel(0)\n",
    "adjectives = tl.loc[idx[:,('JJ')],]\n",
    "adj_dfs = [adjectives for vol in fr_modtrav.volumes()]\n",
    "all_adj = pandas.concat(adj_dfs).groupby(level='token').sum().sort_values('count', ascending=False)[:50] # change 50 here\n",
    "\n",
    "all_adj.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you find? \n",
    "\n",
    "**Here is a list of the codes in the Penn Treebank: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html. Try editing the code to retrieve tokens of another part of speech by changing the code `'JJ'`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pandas.IndexSlice\n",
    "vol = next(fr_modtrav.volumes())\n",
    "tl = vol.tokenlist(pages=False)\n",
    "tl.index = tl.index.droplevel(0)\n",
    "pos = tl.loc[idx[:,('ADD POS CODE HERE')],]\n",
    "pos_dfs = [adjectives for vol in fr_modtrav.volumes()]\n",
    "all_pos = pandas.concat(adj_dfs).groupby(level='token').sum().sort_values('count', ascending=False)[:50]\n",
    "\n",
    "all_pos.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Additional Activity: more frequency questions\n",
    "Let's combine what we've learned and written to try to answer some specific questions. First, we'll define a number of variables that we will use later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1 = fr_modtrav.first() # the first volume in our FR collection\n",
    "tokens = vol1.tokenlist(pages=False, pos=False, case=False) # a token list created from that volume\n",
    "tokens = tokens.loc['body'] # only focus on section='body'\n",
    "\n",
    "# removing stop words and non-alphabetical characters, such as punctuation and numbers:\n",
    "subset1 = tokens[~tokens.index.isin(stopwords) & tokens.index.str.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on proper nouns, here reflected as the code `NNP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol1.tokenlist(pages=False, pos=True, case=True).loc['body']\n",
    "proper_nouns_v1 = tokens.loc[(slice(None), ('NNP')),].sort_values('count', ascending=False) # Select NNP and sort\n",
    "proper_nouns_v1['rel_freq'] = proper_nouns_v1['count'] / proper_nouns_v1['count'].sum() # Calculate Relative frequency\n",
    "proper_nouns_v1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of relative frequencies is that it is easy to compare multiple books. So let's load another volume of  _The Modern Traveller_ to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2 = FeatureReader('ef-data/mdp.39015073767751.json.bz2').first()\n",
    "tokens = vol2.tokenlist(pages=False, pos=True, case=True).loc['body']\n",
    "proper_nouns_v2 = tokens.loc[(slice(None), ('NNP')),].sort_values('count', ascending=False)\n",
    "proper_nouns_v2['rel_freq'] = proper_nouns_v2['count'] / proper_nouns_v2['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proper_nouns_v2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the index of ```token``` and ```pos``` is the same, you can just subtract one DataFrame from another and the code will know to align the rows (i.e. subtracting the `(token, pos)` information). We can do this to see which tokens increased in usage from volume 1 to volume 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vols = (proper_nouns_v2 - proper_nouns_v1)\n",
    "compare_vols.sort_values('rel_freq', ascending=False).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the name of minimizing hassle, we've used our _The Modern Traveller_ workset again for this, but it's admittedly not the most interesting workset for this type of question, as each volume is focused on a different region of the world. Because of this, the results here are pretty straightforward: generally the most frequent proper nouns in volume 1 have the largest drop in frequency when compared to volume 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
